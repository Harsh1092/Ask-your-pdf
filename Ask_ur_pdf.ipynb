{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "5NSQY_SwvBL9",
        "outputId": "8157eaa5-dc4e-4b0d-ae8e-a530d08e19ef"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'streamlit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-df48ae268105>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfitz\u001b[0m  \u001b[0;31m# PyMuPDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "import streamlit as st\n",
        "import PyPDF2\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Streamlit page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"Document Q&A\",\n",
        "    page_icon=\"üìö\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Import necessary libraries\n",
        "from dotenv import load_dotenv\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "class DocumentAnalyzer:\n",
        "    @staticmethod\n",
        "    def analyze_pdf(pdf_path):\n",
        "        \"\"\"\n",
        "        Comprehensive PDF analysis\n",
        "\n",
        "        Args:\n",
        "            pdf_path (str): Path to the PDF file\n",
        "\n",
        "        Returns:\n",
        "            dict: Analysis results\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Open the PDF\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "                # Basic document info\n",
        "                analysis = {\n",
        "                    'filename': os.path.basename(pdf_path),\n",
        "                    'total_pages': len(pdf_reader.pages),\n",
        "                    'total_words': 0,\n",
        "                    'file_size': os.path.getsize(pdf_path) / 1024  # size in KB\n",
        "                }\n",
        "\n",
        "            # Use PyMuPDF for more detailed analysis\n",
        "            pdf_document = fitz.open(pdf_path)\n",
        "\n",
        "            # Additional analysis\n",
        "            analysis['images'] = 0\n",
        "            analysis['headings'] = []\n",
        "\n",
        "            for page_num in range(len(pdf_document)):\n",
        "                page = pdf_document[page_num]\n",
        "\n",
        "                # Count images\n",
        "                images = page.get_images()\n",
        "                analysis['images'] += len(images)\n",
        "\n",
        "                # Extract text and count words\n",
        "                text = page.get_text()\n",
        "                analysis['total_words'] += len(text.split())\n",
        "\n",
        "                # Simple heading extraction\n",
        "                blocks = page.get_text(\"blocks\")\n",
        "                for block in blocks:\n",
        "                    if block[6] > 12:  # font size > 12\n",
        "                        heading = block[4].strip()\n",
        "                        if heading and heading not in analysis['headings']:\n",
        "                            analysis['headings'].append(heading)\n",
        "\n",
        "            # Close the document\n",
        "            pdf_document.close()\n",
        "\n",
        "            return analysis\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error analyzing PDF {pdf_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "class DocumentQAApp:\n",
        "    def __init__(self):\n",
        "        # Initialize session state\n",
        "        if 'vectors' not in st.session_state:\n",
        "            st.session_state.vectors = None\n",
        "        if 'document_analysis' not in st.session_state:\n",
        "            st.session_state.document_analysis = []\n",
        "\n",
        "        # Initialize LLM\n",
        "        self.setup_llm()\n",
        "\n",
        "        # Setup prompt template\n",
        "        self.setup_prompt()\n",
        "\n",
        "    def setup_llm(self):\n",
        "        \"\"\"Initialize Language Model\"\"\"\n",
        "        try:\n",
        "            # Retrieve API key from environment variable\n",
        "            groq_api_key = os.getenv('GROQ_API_KEY')\n",
        "\n",
        "            if not groq_api_key:\n",
        "                st.error(\"GROQ API Key not found. Please set it in your .env file.\")\n",
        "                st.stop()\n",
        "\n",
        "            self.llm = ChatGroq(\n",
        "                groq_api_key=groq_api_key,\n",
        "                model_name=\"Llama3-8b-8192\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error initializing LLM: {e}\")\n",
        "            st.stop()\n",
        "\n",
        "    def setup_prompt(self):\n",
        "        \"\"\"Create prompt template\"\"\"\n",
        "        self.prompt = ChatPromptTemplate.from_template(\n",
        "            \"\"\"Answer the questions based only on the provided context.\n",
        "            Provide the most accurate response possible.\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            Question: {input}\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    def vector_embedding(self, pdf_directory):\n",
        "        \"\"\"Create vector embeddings from PDF documents\"\"\"\n",
        "        # Validate directory\n",
        "        if not os.path.exists(pdf_directory):\n",
        "            st.error(f\"Directory {pdf_directory} does not exist!\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Find PDF files\n",
        "            pdf_files = [f for f in os.listdir(pdf_directory) if f.lower().endswith('.pdf')]\n",
        "\n",
        "            if not pdf_files:\n",
        "                st.warning(\"No PDF files found in the directory.\")\n",
        "                return False\n",
        "\n",
        "            # Process documents\n",
        "            documents = []\n",
        "            st.session_state.document_analysis = []\n",
        "\n",
        "            for pdf_file in pdf_files:\n",
        "                try:\n",
        "                    pdf_path = os.path.join(pdf_directory, pdf_file)\n",
        "\n",
        "                    # Load PDF\n",
        "                    loader = PyPDFLoader(pdf_path)\n",
        "                    pages = loader.load()\n",
        "                    documents.extend(pages)\n",
        "\n",
        "                    # Analyze PDF\n",
        "                    analysis = DocumentAnalyzer.analyze_pdf(pdf_path)\n",
        "                    if analysis:\n",
        "                        st.session_state.document_analysis.append(analysis)\n",
        "\n",
        "                except Exception as file_error:\n",
        "                    st.error(f\"Error processing {pdf_file}: {file_error}\")\n",
        "\n",
        "            # Check if any documents were loaded\n",
        "            if not documents:\n",
        "                st.error(\"No documents could be loaded.\")\n",
        "                return False\n",
        "\n",
        "            # Text Splitting\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200\n",
        "            )\n",
        "            split_documents = text_splitter.split_documents(documents)\n",
        "\n",
        "            # Embedding\n",
        "            embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        "            )\n",
        "\n",
        "            # Create Vector Store\n",
        "            st.session_state.vectors = FAISS.from_documents(\n",
        "                split_documents,\n",
        "                embeddings\n",
        "            )\n",
        "\n",
        "            st.success(f\"Vector Store created with {len(split_documents)} document chunks.\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Comprehensive Error during vector embedding: {e}\")\n",
        "            import traceback\n",
        "            st.error(traceback.format_exc())\n",
        "            return False\n",
        "\n",
        "    def process_query(self, query):\n",
        "        \"\"\"Process user query using the vector store and language model\"\"\"\n",
        "        try:\n",
        "            # Check if vectors are initialized\n",
        "            if st.session_state.vectors is None:\n",
        "                st.warning(\"Please analyze documents first!\")\n",
        "                return \"No documents have been processed. Click 'Analyze Documents' first.\"\n",
        "\n",
        "            # Perform similarity search\n",
        "            retriever = st.session_state.vectors.as_retriever(\n",
        "                search_kwargs={'k': 3}  # Retrieve top 3 most relevant documents\n",
        "            )\n",
        "            relevant_docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "            # Prepare context\n",
        "            context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "\n",
        "            # Generate response\n",
        "            response = self.llm.invoke(\n",
        "                self.prompt.format_messages(\n",
        "                    context=context,\n",
        "                    input=query\n",
        "                )\n",
        "            )\n",
        "\n",
        "            return response.content\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error processing query: {e}\")\n",
        "            return f\"An error occurred: {e}\"\n",
        "\n",
        "def main():\n",
        "    # Page title\n",
        "    st.title(\"üìö Document Insight AI\")\n",
        "    st.write(\"Smart Document Analysis and Q&A\")\n",
        "\n",
        "    # Initialize the app\n",
        "    app = DocumentQAApp()\n",
        "\n",
        "    # PDF Directory Input\n",
        "    pdf_directory = st.text_input(\n",
        "        \"Enter PDF Directory Path\",\n",
        "        placeholder=\"C:/path/to/your/pdf/folder\"\n",
        "    )\n",
        "\n",
        "    # Analyze Documents Button\n",
        "    if st.button(\"üîç Analyze Documents\"):\n",
        "        if not pdf_directory:\n",
        "            st.warning(\"Please enter a directory path.\")\n",
        "        else:\n",
        "            with st.spinner('Processing Documents...'):\n",
        "                if app.vector_embedding(pdf_directory):\n",
        "                    # Display document analysis\n",
        "                    st.subheader(\"üìä Document Analysis\")\n",
        "                    for doc_info in st.session_state.document_analysis:\n",
        "                        st.write(f\"üìÑ **{doc_info['filename']}**\")\n",
        "                        col1, col2 = st.columns(2)\n",
        "                        with col1:\n",
        "                            st.write(f\"Total Pages: {doc_info['total_pages']}\")\n",
        "                            st.write(f\"Total Words: {doc_info['total_words']}\")\n",
        "                        with col2:\n",
        "                            st.write(f\"File Size: {doc_info['file_size']:.2f} KB\")\n",
        "                            st.write(f\"Images: {doc_info.get('images', 0)}\")\n",
        "\n",
        "    # Query Section\n",
        "    st.subheader(\"üí¨ Ask Your Question\")\n",
        "    query = st.text_input(\"Enter your question\")\n",
        "\n",
        "    # Query Processing\n",
        "    if query:\n",
        "        with st.spinner('Generating Response...'):\n",
        "            answer = app.process_query(query)\n",
        "            st.write(\"ü§ñ **AI Response:**\")\n",
        "            st.write(answer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}